{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aë¬¼ë¥˜ì„¼í„° (purchase & sales)"
      ],
      "metadata": {
        "id": "54wRdVCfXZ0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combined Purchase & Sales Data Preprocessing Pipeline\n",
        "# Integrates best practices from both preprocessing approaches\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from difflib import get_close_matches\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: CLASSIFICATION KEY CREATION (from File 1 - more sophisticated)\n",
        "# ============================================================================\n",
        "\n",
        "def make_main_key(name):\n",
        "    \"\"\"\n",
        "    Creates a standardized classification key by cleaning product names.\n",
        "    Removes numbers, units, special characters, and brackets.\n",
        "    \"\"\"\n",
        "    if pd.isna(name):\n",
        "        return None\n",
        "\n",
        "    s = unicodedata.normalize(\"NFKC\", str(name)).lower()\n",
        "    # Remove content in brackets\n",
        "    s = re.sub(r\"[\\(\\[\\{<].*?[\\)\\]\\}>]\", \"\", s)\n",
        "    # Remove number patterns with units\n",
        "    s = re.sub(r\"\\b\\d+(?:\\.\\d+)?\\s*([xÃ—*]\\s*\\d+)\\s*(ea|cs|set|íŒ©|ê°œ|ì…)?\", \"\", s)\n",
        "    # Remove units\n",
        "    unit = r\"(ml|mL|l|â„“|g|kg|mg|oz|lb|mm|cm|m|ea|pcs?|pack|box|set|sheet|roll|í¬|ë´‰|ìº”|ë³‘|íŒ©|ê°œ|ì…|ë§¤|ë§¤ì…|ì„¸íŠ¸)\"\n",
        "    s = re.sub(rf\"\\b\\d+(?:\\.\\d+)?\\s*{unit}\\b\", \"\", s, flags=re.IGNORECASE)\n",
        "    # Remove remaining numbers\n",
        "    s = re.sub(r\"\\b\\d+(?:\\.\\d+)?\", \"\", s)\n",
        "    # Remove special characters\n",
        "    s = re.sub(r\"[\\[\\]<>ï¼œï¼/&+/Ã—*_\\-~Â·â€¢:;|,.!?\\^\\\"\\'\\`]\", \"\", s)\n",
        "    # Remove extra spaces\n",
        "    s = re.sub(r\"\\s+\", \"\", s).strip()\n",
        "\n",
        "    return s if s else None\n",
        "\n",
        "\n",
        "def split_container_key(df, key_col='ë¶„ë¥˜í‚¤'):\n",
        "    \"\"\"\n",
        "    Identifies container products and adds '_ìš©ê¸°' suffix to their classification key.\n",
        "    Uses strict patterns and food exclusion rules.\n",
        "    \"\"\"\n",
        "    # Strict container patterns\n",
        "    container_strict_pat = r\"(ë°€íìš©ê¸°|ë³´ê´€ìš©ê¸°|ë°˜ì°¬í†µ|ì°¬í•©|ë„ì‹œë½í†µ|ë½ì•¤ë½|íŠ¸ë¼ì´íƒ„|ë³´ê´€íƒ‘|ë³´ê´€ë³‘|ë³´ì˜¨ë„ì‹œë½|ë³´ì˜¨ë³‘|ë³´ëƒ‰ë°±|ì¿¨ëŸ¬ë°±|í‘¸ë“œì»¨í…Œì´ë„ˆ|ì§€í¼ë°±|ì§€í¼ë½|íŠ¸ë ˆì´|ë³´ê´€í•¨|ìœ ë¦¬ë°€í|ìŠ¤í…ë°€í|ìŠ¤í…Œì¸ë¦¬ìŠ¤ìš©ê¸°|ê¹€ì¹˜í†µ|ì–‘ë…í†µ|ë¶„ìœ ì»¨í…Œì´ë„ˆ|ìˆ˜ì €í†µ|ì–‘ë…ë³‘)\"\n",
        "\n",
        "    # Food patterns to exclude\n",
        "    food_pat = r\"(ê»Œ|ì ¤ë¦¬í†¨|ìº”ë””|ì‚¬íƒ•|ì´ˆì½œë¦¿|ì´ˆì½œ|ë¹„ìŠ¤í‚·|ì¿ í‚¤|ì ¤ë¦¬|ì¹´ë¼ë©œ|ì¸„ì‰|ì ¤|ìŠ¤ë‚µ|ë¼ë©´|ìš°ë™|êµ­ìˆ˜|ë©´|ê³¼ì|ìŒë£Œ|ì»¤í”¼|ì°¨|ì‹œë¦¬ì–¼|ëˆ„ë£½ì§€)\"\n",
        "\n",
        "    # Match strict container patterns\n",
        "    m_strict = df[\"ìƒí’ˆëª…\"].str.contains(container_strict_pat, regex=True, na=False)\n",
        "\n",
        "    # Match loose container pattern (contains \"ìš©ê¸°\") but not food\n",
        "    m_loose = df[\"ìƒí’ˆëª…\"].str.contains(r\"ìš©ê¸°\", regex=True, na=False) & ~df[\"ìƒí’ˆëª…\"].str.contains(food_pat, regex=True, na=False)\n",
        "\n",
        "    # Combine matches and exclude food\n",
        "    mask_container = (m_strict | m_loose) & ~df[\"ìƒí’ˆëª…\"].str.contains(food_pat, regex=True, na=False)\n",
        "\n",
        "    # Add '_ìš©ê¸°' suffix\n",
        "    df.loc[mask_container, key_col] = df.loc[mask_container, key_col] + \"_ìš©ê¸°\"\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: AUTOMATED CLASSIFICATION FILLING (from File 1)\n",
        "# ============================================================================\n",
        "\n",
        "def fill_main_by_key(df, key_col, label_col, min_support=3, purity=0.9,\n",
        "                     restrict_to_existing=True, fallback_value=None):\n",
        "    \"\"\"\n",
        "    Automatically fills missing classifications based on key patterns.\n",
        "    Uses statistical validation with minimum support and purity thresholds.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # Calculate frequency of (key, label) pairs\n",
        "    g = (out.dropna(subset=[key_col, label_col])\n",
        "            .groupby([key_col, label_col]).size()\n",
        "            .rename('cnt').reset_index())\n",
        "\n",
        "    if g.empty:\n",
        "        return out, {}, pd.DataFrame()\n",
        "\n",
        "    # Calculate total count per key and most frequent label\n",
        "    tot = g.groupby(key_col)['cnt'].sum().rename('tot')\n",
        "    top = (g.sort_values(['cnt'], ascending=False)\n",
        "                .groupby(key_col)\n",
        "                .head(1)\n",
        "                .set_index(key_col))\n",
        "\n",
        "    stat = top.join(tot).reset_index()\n",
        "    stat['purity'] = stat['cnt'] / stat['tot']\n",
        "\n",
        "    # Filter by minimum support and purity\n",
        "    ok = stat[(stat['tot'] >= min_support) & (stat['purity'] >= purity)]\n",
        "    key2label = dict(zip(ok[key_col], ok[label_col]))\n",
        "\n",
        "    # Restrict to existing classification values\n",
        "    if restrict_to_existing:\n",
        "        candidates = df[label_col].dropna().unique().tolist()\n",
        "        new_key2label = {}\n",
        "\n",
        "        for k, v in key2label.items():\n",
        "            if v in candidates:\n",
        "                new_key2label[k] = v\n",
        "            else:\n",
        "                # Try fuzzy matching\n",
        "                match = get_close_matches(v, candidates, n=1, cutoff=0.6)\n",
        "                if match:\n",
        "                    new_key2label[k] = match[0]\n",
        "                elif fallback_value is not None:\n",
        "                    new_key2label[k] = fallback_value\n",
        "\n",
        "        key2label = new_key2label\n",
        "\n",
        "    # Fill missing values\n",
        "    mask = out[label_col].isna()\n",
        "    out.loc[mask, label_col] = out.loc[mask, key_col].map(key2label)\n",
        "\n",
        "    # Create statistics report\n",
        "    report = (stat.assign(adopted=stat[key_col].isin(ok[key_col]))\n",
        "                    .sort_values(['adopted', 'purity', 'tot'],\n",
        "                                ascending=[True, False, False]))\n",
        "\n",
        "    return out, key2label, report\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: BRAND PREFIX ANALYSIS (from File 2)\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_brand_prefixes(df, subcol='ì†Œë¶„ë¥˜'):\n",
        "    \"\"\"\n",
        "    Analyzes brand prefixes (text before ']') in product names.\n",
        "    Returns statistics on how prefixes map to subcategories.\n",
        "    \"\"\"\n",
        "    pat = re.compile(r'^\\s*([^]]+)]')\n",
        "\n",
        "    # Extract prefixes from non-null subcategories\n",
        "    with_sub = df[df[subcol].notna()].copy()\n",
        "    prefixes = set()\n",
        "\n",
        "    for name in with_sub['ìƒí’ˆëª…'].dropna():\n",
        "        m = pat.match(name)\n",
        "        if m:\n",
        "            prefixes.add(m.group(1).strip())\n",
        "\n",
        "    # Analyze distribution\n",
        "    stats = []\n",
        "    for prefix in sorted(prefixes):\n",
        "        regex = re.compile(rf'(^|\\s){re.escape(prefix)}\\]')\n",
        "        matched = with_sub[with_sub['ìƒí’ˆëª…'].str.contains(regex, na=False)]\n",
        "\n",
        "        if not matched.empty:\n",
        "            counts = matched[subcol].value_counts()\n",
        "            for cat, cnt in counts.items():\n",
        "                stats.append({\n",
        "                    'ë¸Œëœë“œ': prefix,\n",
        "                    'ì†Œë¶„ë¥˜': cat,\n",
        "                    'ìƒí’ˆëª…_ê°œìˆ˜': cnt\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(stats).sort_values(['ë¸Œëœë“œ', 'ìƒí’ˆëª…_ê°œìˆ˜'],\n",
        "                                          ascending=[True, False])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: SALES DATA HARMONIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def harmonize_sales_columns(df, dataset_type='2123'):\n",
        "    \"\"\"\n",
        "    Harmonizes column names between 2021-23 and 2024 sales datasets.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Sales dataframe to harmonize\n",
        "    dataset_type : str\n",
        "        '2123' for 2021-23 data, '24' for 2024 data\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    if dataset_type == '2123':\n",
        "        # Rename columns to match standard naming\n",
        "        rename_map = {\n",
        "            'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸': 'ìš°í¸ë²ˆí˜¸',\n",
        "            'ë°”ì½”ë“œ': 'ìƒí’ˆë°”ì½”ë“œ',\n",
        "            'ê³µê¸‰ê¸ˆì•¡': 'ê³µê¸‰ê°€ì•¡',\n",
        "            'ë¶€ê°€ì„¸(ê³¼ì„¸)': 'ë¶€ê°€ì„¸',\n",
        "            'ì˜µì…˜ ì½”ë“œ': 'ì˜µì…˜ì½”ë“œ'\n",
        "        }\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "    elif dataset_type == '24':\n",
        "        # Rename columns to match standard naming\n",
        "        rename_map = {\n",
        "            'ìƒí’ˆ ë°”ì½”ë“œ(ëŒ€í•œìƒì˜)': 'ìƒí’ˆë°”ì½”ë“œ'\n",
        "        }\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_and_merge_sales_data(filepath_2123, filepath_24):\n",
        "    \"\"\"\n",
        "    Loads both sales datasets and merges them into a single dataframe.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    filepath_2123 : str\n",
        "        Path to 2021-23 sales data CSV\n",
        "    filepath_24 : str\n",
        "        Path to 2024 sales data CSV\n",
        "    \"\"\"\n",
        "    print(\"\\n[Loading Sales Data]\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Load 2021-23 data\n",
        "    print(f\"Loading {filepath_2123}...\")\n",
        "    df_2123 = pd.read_csv(filepath_2123, dtype=str)\n",
        "    df_2123 = harmonize_sales_columns(df_2123, dataset_type='2123')\n",
        "    print(f\"   Rows: {len(df_2123):,}\")\n",
        "\n",
        "    # Load 2024 data\n",
        "    print(f\"Loading {filepath_24}...\")\n",
        "    df_24 = pd.read_csv(filepath_24, dtype=str)\n",
        "    df_24 = harmonize_sales_columns(df_24, dataset_type='24')\n",
        "    print(f\"   Rows: {len(df_24):,}\")\n",
        "\n",
        "    # Merge datasets\n",
        "    print(\"\\nMerging datasets...\")\n",
        "    df_merged = pd.concat([df_2123, df_24], ignore_index=True)\n",
        "    print(f\"   Total rows after merge: {len(df_merged):,}\")\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: MAIN PREPROCESSING PIPELINES\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_purchase_data(filepath, target_categories=None):\n",
        "    \"\"\"\n",
        "    Main preprocessing pipeline for purchase data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    filepath : str\n",
        "        Path to the purchase data CSV file\n",
        "    target_categories : list, optional\n",
        "        List of target ì¤‘ë¶„ë¥˜ categories to filter\n",
        "        Default: ['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']\n",
        "    \"\"\"\n",
        "\n",
        "    if target_categories is None:\n",
        "        target_categories = ['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"PURCHASE DATA PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Load data\n",
        "    print(\"\\n[Step 1] Loading data...\")\n",
        "    df = pd.read_csv(filepath, dtype=str)\n",
        "    print(f\"   Initial rows: {len(df):,}\")\n",
        "\n",
        "    # Step 2: Remove returns\n",
        "    print(\"\\n[Step 2] Removing return entries (ë°˜ì¶œ)...\")\n",
        "    df = df[df['ì‘ì—…ìœ í˜•'] != 'ë°˜ì¶œ'].copy()\n",
        "    print(f\"   Rows after removal: {len(df):,}\")\n",
        "\n",
        "    # Step 3: Create classification key\n",
        "    print(\"\\n[Step 3] Creating classification keys...\")\n",
        "    df['ë¶„ë¥˜í‚¤'] = df['ìƒí’ˆëª…'].apply(make_main_key)\n",
        "    df = split_container_key(df, key_col='ë¶„ë¥˜í‚¤')\n",
        "    print(f\"   Unique keys created: {df['ë¶„ë¥˜í‚¤'].nunique():,}\")\n",
        "\n",
        "    # Step 4: Fill missing classifications (initial pass - groupby)\n",
        "    print(\"\\n[Step 4] Initial classification filling (groupby method)...\")\n",
        "    for col in [\"ëŒ€ë¶„ë¥˜\", \"ì¤‘ë¶„ë¥˜\", \"ì†Œë¶„ë¥˜\"]:\n",
        "        before = df[col].isna().sum()\n",
        "        mask = df[col].isna()\n",
        "        df.loc[mask, col] = (\n",
        "            df.groupby(\"ë¶„ë¥˜í‚¤\")[col]\n",
        "            .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "        )[mask]\n",
        "        after = df[col].isna().sum()\n",
        "        print(f\"   {col}: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 5: Fill missing barcodes\n",
        "    print(\"\\n[Step 5] Filling missing barcodes...\")\n",
        "    if \"ë°”ì½”ë“œ\" in df.columns:\n",
        "        before = df[\"ë°”ì½”ë“œ\"].isna().sum()\n",
        "        mask = df[\"ë°”ì½”ë“œ\"].isna()\n",
        "        if \"ìƒí’ˆì½”ë“œ\" in df.columns:\n",
        "            df.loc[mask, \"ë°”ì½”ë“œ\"] = (\n",
        "                df.groupby(\"ìƒí’ˆì½”ë“œ\")[\"ë°”ì½”ë“œ\"]\n",
        "                .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "            )[mask]\n",
        "        after = df[\"ë°”ì½”ë“œ\"].isna().sum()\n",
        "        print(f\"   Barcodes: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 6: Filter by target categories\n",
        "    print(f\"\\n[Step 6] Filtering by target categories: {target_categories}...\")\n",
        "    df_filtered = df[df['ì¤‘ë¶„ë¥˜'].isin(target_categories)].copy()\n",
        "    print(f\"   Rows after filtering: {len(df_filtered):,}\")\n",
        "    print(f\"   Unique products: {df_filtered['ìƒí’ˆëª…'].nunique():,}\")\n",
        "\n",
        "    # Step 7: Automated classification filling (advanced)\n",
        "    print(\"\\n[Step 7] Advanced classification filling...\")\n",
        "    df_updated = df_filtered.copy()\n",
        "\n",
        "    # Fill ëŒ€ë¶„ë¥˜\n",
        "    before = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    df_filled, key2label, report = fill_main_by_key(\n",
        "        df_updated,\n",
        "        key_col='ë¶„ë¥˜í‚¤',\n",
        "        label_col='ëŒ€ë¶„ë¥˜',\n",
        "        min_support=2,\n",
        "        purity=0.7\n",
        "    )\n",
        "\n",
        "    fill_map = df_filled.dropna(subset=['ëŒ€ë¶„ë¥˜']).groupby('ë¶„ë¥˜í‚¤')['ëŒ€ë¶„ë¥˜'].first()\n",
        "    mask = df_updated['ëŒ€ë¶„ë¥˜'].isna()\n",
        "    df_updated.loc[mask, 'ëŒ€ë¶„ë¥˜'] = df_updated.loc[mask, 'ë¶„ë¥˜í‚¤'].map(fill_map)\n",
        "    after = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    print(f\"   ëŒ€ë¶„ë¥˜: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 8: Analyze and handle unclassified data\n",
        "    print(\"\\n[Step 8] Analyzing unclassified data...\")\n",
        "    unclassified = df_updated[df_updated['ì†Œë¶„ë¥˜'].isna()]\n",
        "    print(f\"   Unclassified rows: {len(unclassified):,}\")\n",
        "\n",
        "    if len(unclassified) > 0:\n",
        "        # Fill remaining with default category\n",
        "        default_category = 'ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹'\n",
        "        mask = df_updated['ì†Œë¶„ë¥˜'].isna()\n",
        "        df_updated.loc[mask, 'ì†Œë¶„ë¥˜'] = default_category\n",
        "        print(f\"   Filled remaining with default: '{default_category}'\")\n",
        "\n",
        "    # Step 9: Clean up columns\n",
        "    print(\"\\n[Step 9] Cleaning up columns...\")\n",
        "    columns_to_drop = [\n",
        "        'ì‘ì—…ìœ í˜•', 'ë§¤ì¶œì²˜ì½”ë“œ', 'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸', 'ì…ê³  í˜•íƒœ',\n",
        "        'ë°”ì½”ë“œ', 'ê·œê²©', 'ì…ìˆ˜', 'ìˆ˜ëŸ‰', 'íŒë§¤ê¸ˆì•¡', 'ë¶€ê°€ì„¸(ê³¼ì„¸)',\n",
        "        'ìƒí’ˆì½”ë“œ', 'ì˜µì…˜ ì½”ë“œ', 'ì˜µì…˜', 'ëŒ€ë¶„ë¥˜'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_updated.columns]\n",
        "    df_updated = df_updated.drop(columns_to_drop, axis=1)\n",
        "    print(f\"   Dropped {len(columns_to_drop)} columns\")\n",
        "    print(f\"   Remaining columns: {list(df_updated.columns)}\")\n",
        "\n",
        "    # Step 10: Final cleanup\n",
        "    print(\"\\n[Step 10] Final cleanup...\")\n",
        "    df_updated = df_updated.reset_index(drop=True)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPROCESSING COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nFinal dataset shape: {df_updated.shape}\")\n",
        "    print(f\"Rows: {len(df_updated):,}\")\n",
        "    print(f\"Columns: {len(df_updated.columns)}\")\n",
        "    print(f\"\\nMissing values per column:\")\n",
        "    print(df_updated.isnull().sum())\n",
        "    print(f\"\\nUnique values per classification:\")\n",
        "    print(f\"  ì¤‘ë¶„ë¥˜: {df_updated['ì¤‘ë¶„ë¥˜'].nunique()}\")\n",
        "    print(f\"  ì†Œë¶„ë¥˜: {df_updated['ì†Œë¶„ë¥˜'].nunique()}\")\n",
        "\n",
        "    return df_updated\n",
        "\n",
        "\n",
        "def preprocess_sales_data(filepath_2123, filepath_24, target_categories=None):\n",
        "    \"\"\"\n",
        "    Main preprocessing pipeline for sales data (combines 2021-23 and 2024 data).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    filepath_2123 : str\n",
        "        Path to the 2021-23 sales data CSV file\n",
        "    filepath_24 : str\n",
        "        Path to the 2024 sales data CSV file\n",
        "    target_categories : list, optional\n",
        "        List of target ì¤‘ë¶„ë¥˜ categories to filter\n",
        "        Default: ['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']\n",
        "    \"\"\"\n",
        "\n",
        "    if target_categories is None:\n",
        "        target_categories = ['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SALES DATA PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Load and merge data\n",
        "    df = load_and_merge_sales_data(filepath_2123, filepath_24)\n",
        "\n",
        "    # Step 2: Create classification key\n",
        "    print(\"\\n[Step 2] Creating classification keys...\")\n",
        "    df['ë¶„ë¥˜í‚¤'] = df['ìƒí’ˆëª…'].apply(make_main_key)\n",
        "    df = split_container_key(df, key_col='ë¶„ë¥˜í‚¤')\n",
        "    print(f\"   Unique keys created: {df['ë¶„ë¥˜í‚¤'].nunique():,}\")\n",
        "\n",
        "    # Step 3: Fill missing classifications (initial pass - groupby)\n",
        "    print(\"\\n[Step 3] Initial classification filling (groupby method)...\")\n",
        "    for col in [\"ëŒ€ë¶„ë¥˜\", \"ì¤‘ë¶„ë¥˜\", \"ì†Œë¶„ë¥˜\"]:\n",
        "        before = df[col].isna().sum()\n",
        "        mask = df[col].isna()\n",
        "        df.loc[mask, col] = (\n",
        "            df.groupby(\"ë¶„ë¥˜í‚¤\")[col]\n",
        "            .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "        )[mask]\n",
        "        after = df[col].isna().sum()\n",
        "        print(f\"   {col}: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 4: Fill missing barcodes\n",
        "    print(\"\\n[Step 4] Filling missing barcodes...\")\n",
        "    if \"ìƒí’ˆë°”ì½”ë“œ\" in df.columns:\n",
        "        before = df[\"ìƒí’ˆë°”ì½”ë“œ\"].isna().sum()\n",
        "        mask = df[\"ìƒí’ˆë°”ì½”ë“œ\"].isna()\n",
        "        df.loc[mask, \"ìƒí’ˆë°”ì½”ë“œ\"] = (\n",
        "            df.groupby(\"ìƒí’ˆëª…\")[\"ìƒí’ˆë°”ì½”ë“œ\"]\n",
        "            .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "        )[mask]\n",
        "        after = df[\"ìƒí’ˆë°”ì½”ë“œ\"].isna().sum()\n",
        "        print(f\"   Barcodes: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 5: Filter by target categories\n",
        "    print(f\"\\n[Step 5] Filtering by target categories: {target_categories}...\")\n",
        "    df_filtered = df[df['ì¤‘ë¶„ë¥˜'].isin(target_categories)].copy()\n",
        "    print(f\"   Rows after filtering: {len(df_filtered):,}\")\n",
        "    print(f\"   Unique products: {df_filtered['ìƒí’ˆëª…'].nunique():,}\")\n",
        "\n",
        "    # Step 6: Automated classification filling (advanced)\n",
        "    print(\"\\n[Step 6] Advanced classification filling...\")\n",
        "    df_updated = df_filtered.copy()\n",
        "\n",
        "    # Fill ëŒ€ë¶„ë¥˜\n",
        "    before = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    df_filled, key2label, report = fill_main_by_key(\n",
        "        df_updated,\n",
        "        key_col='ë¶„ë¥˜í‚¤',\n",
        "        label_col='ëŒ€ë¶„ë¥˜',\n",
        "        min_support=2,\n",
        "        purity=0.7\n",
        "    )\n",
        "\n",
        "    fill_map = df_filled.dropna(subset=['ëŒ€ë¶„ë¥˜']).groupby('ë¶„ë¥˜í‚¤')['ëŒ€ë¶„ë¥˜'].first()\n",
        "    mask = df_updated['ëŒ€ë¶„ë¥˜'].isna()\n",
        "    df_updated.loc[mask, 'ëŒ€ë¶„ë¥˜'] = df_updated.loc[mask, 'ë¶„ë¥˜í‚¤'].map(fill_map)\n",
        "    after = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    print(f\"   ëŒ€ë¶„ë¥˜: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 7: Analyze and handle unclassified data\n",
        "    print(\"\\n[Step 7] Analyzing unclassified data...\")\n",
        "    unclassified = df_updated[df_updated['ì†Œë¶„ë¥˜'].isna()]\n",
        "    print(f\"   Unclassified rows: {len(unclassified):,}\")\n",
        "\n",
        "    if len(unclassified) > 0:\n",
        "        # Fill remaining with default category\n",
        "        default_category = 'ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹'\n",
        "        mask = df_updated['ì†Œë¶„ë¥˜'].isna()\n",
        "        df_updated.loc[mask, 'ì†Œë¶„ë¥˜'] = default_category\n",
        "        print(f\"   Filled remaining with default: '{default_category}'\")\n",
        "\n",
        "    # Step 8: Clean up columns\n",
        "    print(\"\\n[Step 8] Cleaning up columns...\")\n",
        "    columns_to_drop = [\n",
        "        'êµ¬ë¶„', 'ë§¤ì¶œì²˜ì½”ë“œ', 'ìš°í¸ë²ˆí˜¸', 'ì˜µì…˜ì½”ë“œ',\n",
        "        'ê·œê²©', 'ì…ìˆ˜', 'ìƒí’ˆë°”ì½”ë“œ', 'ëŒ€ë¶„ë¥˜'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_updated.columns]\n",
        "    df_updated = df_updated.drop(columns_to_drop, axis=1)\n",
        "    print(f\"   Dropped {len(columns_to_drop)} columns\")\n",
        "    print(f\"   Remaining columns: {list(df_updated.columns)}\")\n",
        "\n",
        "    # Step 9: Final cleanup\n",
        "    print(\"\\n[Step 9] Final cleanup...\")\n",
        "    df_updated = df_updated.reset_index(drop=True)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPROCESSING COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nFinal dataset shape: {df_updated.shape}\")\n",
        "    print(f\"Rows: {len(df_updated):,}\")\n",
        "    print(f\"Columns: {len(df_updated.columns)}\")\n",
        "    print(f\"\\nMissing values per column:\")\n",
        "    print(df_updated.isnull().sum())\n",
        "    print(f\"\\nUnique values per classification:\")\n",
        "    print(f\"  ì¤‘ë¶„ë¥˜: {df_updated['ì¤‘ë¶„ë¥˜'].nunique()}\")\n",
        "    print(f\"  ì†Œë¶„ë¥˜: {df_updated['ì†Œë¶„ë¥˜'].nunique()}\")\n",
        "\n",
        "    return df_updated\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ========================================================================\n",
        "    # Process PURCHASE data\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"ğŸ”¹\" * 40)\n",
        "    print(\"PROCESSING PURCHASE DATA\")\n",
        "    print(\"ğŸ”¹\" * 40)\n",
        "\n",
        "    a_purchase_processed = preprocess_purchase_data(\n",
        "        'A_purchase_data.csv',\n",
        "        target_categories=['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']\n",
        "    )\n",
        "\n",
        "    # Save processed purchase data\n",
        "    # purchase_output = 'A_purchase_data_processed.csv'\n",
        "    # a_purchase_processed.to_csv(purchase_output, index=False, encoding='utf-8-sig')\n",
        "    # print(f\"\\nâœ… Processed purchase data saved to: {purchase_output}\")\n",
        "\n",
        "    print(\"\\nSample of processed purchase data:\")\n",
        "    print(a_purchase_processed.head(10))\n",
        "\n",
        "    # ========================================================================\n",
        "    # Process SALES data\n",
        "    # ========================================================================\n",
        "    print(\"\\n\\n\" + \"ğŸ”¹\" * 40)\n",
        "    print(\"PROCESSING SALES DATA\")\n",
        "    print(\"ğŸ”¹\" * 40)\n",
        "\n",
        "    a_sales_processed = preprocess_sales_data(\n",
        "        'a_sales_2123.csv',\n",
        "        'a_sales_24.csv',\n",
        "        target_categories=['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']\n",
        "    )\n",
        "\n",
        "    # Save processed sales data\n",
        "    # sales_output = 'A_sales_data_processed.csv'\n",
        "    # a_sales_processed.to_csv(sales_output, index=False, encoding='utf-8-sig')\n",
        "    # print(f\"\\nâœ… Processed sales data saved to: {sales_output}\")\n",
        "\n",
        "    print(\"\\nSample of processed sales data:\")\n",
        "    print(a_sales_processed.head(10))\n",
        "\n",
        "    # ========================================================================\n",
        "    # Final Summary\n",
        "    # ========================================================================\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"ğŸ‰ ALL PREPROCESSING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nğŸ“Š Purchase Data: {len(a_purchase_processed):,} rows\")\n",
        "    print(f\"ğŸ“Š Sales Data: {len(a_sales_processed):,} rows\")\n",
        "    print(f\"\\nğŸ’¾ Output files:\")\n",
        "    # print(f\"   - {purchase_output}\")\n",
        "    # print(f\"   - {sales_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LAFw2kBV4HZ",
        "outputId": "96314581-debb-41f5-aeba-96786d78e01f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "PROCESSING PURCHASE DATA\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "================================================================================\n",
            "PURCHASE DATA PREPROCESSING PIPELINE\n",
            "================================================================================\n",
            "\n",
            "[Step 1] Loading data...\n",
            "   Initial rows: 227,534\n",
            "\n",
            "[Step 2] Removing return entries (ë°˜ì¶œ)...\n",
            "   Rows after removal: 201,534\n",
            "\n",
            "[Step 3] Creating classification keys...\n",
            "   Unique keys created: 6,170\n",
            "\n",
            "[Step 4] Initial classification filling (groupby method)...\n",
            "   ëŒ€ë¶„ë¥˜: 1,623 â†’ 571 missing (1,052 filled)\n",
            "   ì¤‘ë¶„ë¥˜: 1,845 â†’ 690 missing (1,155 filled)\n",
            "   ì†Œë¶„ë¥˜: 113,776 â†’ 106,776 missing (7,000 filled)\n",
            "\n",
            "[Step 5] Filling missing barcodes...\n",
            "   Barcodes: 584 â†’ 525 missing (59 filled)\n",
            "\n",
            "[Step 6] Filtering by target categories: ['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']...\n",
            "   Rows after filtering: 28,295\n",
            "   Unique products: 1,249\n",
            "\n",
            "[Step 7] Advanced classification filling...\n",
            "   ëŒ€ë¶„ë¥˜: 0 â†’ 0 missing (0 filled)\n",
            "\n",
            "[Step 8] Analyzing unclassified data...\n",
            "   Unclassified rows: 20,267\n",
            "   Filled remaining with default: 'ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹'\n",
            "\n",
            "[Step 9] Cleaning up columns...\n",
            "   Dropped 14 columns\n",
            "   Remaining columns: ['ì¼ì', 'ê³µê¸‰ì—…ì²´ ì½”ë“œ', 'ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸', 'ìƒí’ˆëª…', 'EA', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ë¶„ë¥˜í‚¤']\n",
            "\n",
            "[Step 10] Final cleanup...\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Final dataset shape: (28295, 8)\n",
            "Rows: 28,295\n",
            "Columns: 8\n",
            "\n",
            "Missing values per column:\n",
            "ì¼ì           0\n",
            "ê³µê¸‰ì—…ì²´ ì½”ë“œ      0\n",
            "ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸    0\n",
            "ìƒí’ˆëª…          0\n",
            "EA           0\n",
            "ì¤‘ë¶„ë¥˜          0\n",
            "ì†Œë¶„ë¥˜          0\n",
            "ë¶„ë¥˜í‚¤          0\n",
            "dtype: int64\n",
            "\n",
            "Unique values per classification:\n",
            "  ì¤‘ë¶„ë¥˜: 2\n",
            "  ì†Œë¶„ë¥˜: 3\n",
            "\n",
            "Sample of processed purchase data:\n",
            "           ì¼ì ê³µê¸‰ì—…ì²´ ì½”ë“œ ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸                     ìƒí’ˆëª…    EA       ì¤‘ë¶„ë¥˜  \\\n",
            "0  2021-01-05    1019     37899    ë°ë¯¸ì†Œë‹¤ ì• í”Œ<340ml*20>XXX   100  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "1  2021-01-05    1019     37899   ì˜¤ì¸ ì¹´]ë°ë¯¸ì†Œë‹¤<ì˜¤ë Œì§€/250ml/ìº”>   300  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "2  2021-01-05    1019     37899   ì˜¤ì¸ ì¹´]ë°ë¯¸ì†Œë‹¤<ì²­í¬ë„/250ml/ìº”>   300  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "3  2021-01-05    1019     37899        ì˜¤ì¸ ì¹´]ì˜¤ë€ì”¨<íŒŒì¸/1.5L>   240  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "4  2021-01-05    1019     37899               ì˜¤ì¸ ì¹´]ì˜¤ë¡œë‚˜ë¯¼ì”¨  1000  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "5  2021-01-05    1019     37899           ì˜¤ì¸ ì¹´]í¬ì¹´ë¦¬<1.5L>   528  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "6  2021-01-05    1019     37899          ì˜¤ì¸ ì¹´]í¬ì¹´ë¦¬<500ml>  1100  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "7  2021-01-05    1019     37899        ì˜¤ì¸ ì¹´]ì˜¤ë€ì”¨<íŒŒì¸/1.5L>   120  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "8  2021-01-05    1027     41496           ì¹ ì„±]ê²Œí† ë ˆì´<1.5L>   120  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "9  2021-01-05    1027     41496  ë¸ëª¬íŠ¸ íŒ©ë§ê³ ë²ˆë“¤<190ml*24>XXX   240  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•   \n",
            "\n",
            "         ì†Œë¶„ë¥˜          ë¶„ë¥˜í‚¤  \n",
            "0  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹    ë°ë¯¸ì†Œë‹¤ì• í”Œxxx  \n",
            "1  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹      ì˜¤ì¸ ì¹´ë°ë¯¸ì†Œë‹¤  \n",
            "2  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹      ì˜¤ì¸ ì¹´ë°ë¯¸ì†Œë‹¤  \n",
            "3  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹       ì˜¤ì¸ ì¹´ì˜¤ë€ì”¨  \n",
            "4  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹     ì˜¤ì¸ ì¹´ì˜¤ë¡œë‚˜ë¯¼ì”¨  \n",
            "5  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹       ì˜¤ì¸ ì¹´í¬ì¹´ë¦¬  \n",
            "6  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹       ì˜¤ì¸ ì¹´í¬ì¹´ë¦¬  \n",
            "7  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹       ì˜¤ì¸ ì¹´ì˜¤ë€ì”¨  \n",
            "8  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹       ì¹ ì„±ê²Œí† ë ˆì´  \n",
            "9  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  ë¸ëª¬íŠ¸íŒ©ë§ê³ ë²ˆë“¤xxx  \n",
            "\n",
            "\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "PROCESSING SALES DATA\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "================================================================================\n",
            "SALES DATA PREPROCESSING PIPELINE\n",
            "================================================================================\n",
            "\n",
            "[Loading Sales Data]\n",
            "--------------------------------------------------------------------------------\n",
            "Loading a_sales_2123.csv...\n",
            "   Rows: 1,048,575\n",
            "Loading a_sales_24.csv...\n",
            "   Rows: 382,476\n",
            "\n",
            "Merging datasets...\n",
            "   Total rows after merge: 1,431,051\n",
            "\n",
            "[Step 2] Creating classification keys...\n",
            "   Unique keys created: 7,747\n",
            "\n",
            "[Step 3] Initial classification filling (groupby method)...\n",
            "   ëŒ€ë¶„ë¥˜: 4,325 â†’ 2,337 missing (1,988 filled)\n",
            "   ì¤‘ë¶„ë¥˜: 224,647 â†’ 15,946 missing (208,701 filled)\n",
            "   ì†Œë¶„ë¥˜: 836,437 â†’ 726,722 missing (109,715 filled)\n",
            "\n",
            "[Step 4] Filling missing barcodes...\n",
            "   Barcodes: 2,591 â†’ 213 missing (2,378 filled)\n",
            "\n",
            "[Step 5] Filtering by target categories: ['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•', 'ì‹ ì„ ì‹í’ˆ']...\n",
            "   Rows after filtering: 229,074\n",
            "   Unique products: 1,269\n",
            "\n",
            "[Step 6] Advanced classification filling...\n",
            "   ëŒ€ë¶„ë¥˜: 0 â†’ 0 missing (0 filled)\n",
            "\n",
            "[Step 7] Analyzing unclassified data...\n",
            "   Unclassified rows: 199,062\n",
            "   Filled remaining with default: 'ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹'\n",
            "\n",
            "[Step 8] Cleaning up columns...\n",
            "   Dropped 8 columns\n",
            "   Remaining columns: ['íŒë§¤ì¼', 'íŒë§¤ìˆ˜ëŸ‰', 'ìƒí’ˆëª…', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸', 'ë¶„ë¥˜í‚¤']\n",
            "\n",
            "[Step 9] Final cleanup...\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Final dataset shape: (229074, 8)\n",
            "Rows: 229,074\n",
            "Columns: 8\n",
            "\n",
            "Missing values per column:\n",
            "íŒë§¤ì¼     0\n",
            "íŒë§¤ìˆ˜ëŸ‰    0\n",
            "ìƒí’ˆëª…     0\n",
            "ì¤‘ë¶„ë¥˜     0\n",
            "ì†Œë¶„ë¥˜     0\n",
            "ê³µê¸‰ê°€ì•¡    0\n",
            "ë¶€ê°€ì„¸     0\n",
            "ë¶„ë¥˜í‚¤     0\n",
            "dtype: int64\n",
            "\n",
            "Unique values per classification:\n",
            "  ì¤‘ë¶„ë¥˜: 2\n",
            "  ì†Œë¶„ë¥˜: 3\n",
            "\n",
            "Sample of processed sales data:\n",
            "          íŒë§¤ì¼ íŒë§¤ìˆ˜ëŸ‰                 ìƒí’ˆëª…       ì¤‘ë¶„ë¥˜        ì†Œë¶„ë¥˜   ê³µê¸‰ê°€ì•¡   ë¶€ê°€ì„¸  \\\n",
            "0  2021-01-04   24         ì½”ì¹´ì½œë¼<500ml>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  29500  2950   \n",
            "1  2021-01-04   12       ì½”ì¹´]ìŠ¤í”„ë¼ì´íŠ¸<1.5>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  17400  1740   \n",
            "2  2021-01-04    1       ì½”ì¹´ì½œë¼<250ml/ìº”>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  20100  2010   \n",
            "3  2021-01-04    5        ë†ì‹¬ìŒë£Œ]ë°±ì‚°ìˆ˜<2L>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  19600  1960   \n",
            "4  2021-01-04   10          ê°€ì•¼ì‚°ì²œë…„ìˆ˜<2L>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  17300  1730   \n",
            "5  2021-01-04    1         ì½”ì¹´]í† ë ˆíƒ€<1.5>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  16000  1600   \n",
            "6  2021-01-04    2  íŒ”ë„]ë¹„ë½ì‹í˜œìº”<238ml>XXX  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹   9000   900   \n",
            "7  2021-01-04   20        ì¹ ì„±ì‚¬ì´ë‹¤<500ml>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  20000  2000   \n",
            "8  2021-01-04   24         ì½”ì¹´ì½œë¼<500ml>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  29800  2980   \n",
            "9  2021-01-04    1        ì½”ì¹´]ì•”ë°”ì‚¬<1.5L>  ìƒìˆ˜,ìŒë£Œ,ê±´ê°•  ëƒ‰ë™,ëƒ‰ì¥,ê°„í¸ì‹  14700  1470   \n",
            "\n",
            "          ë¶„ë¥˜í‚¤  \n",
            "0        ì½”ì¹´ì½œë¼  \n",
            "1     ì½”ì¹´ìŠ¤í”„ë¼ì´íŠ¸  \n",
            "2        ì½”ì¹´ì½œë¼  \n",
            "3     ë†ì‹¬ìŒë£Œë°±ì‚°ìˆ˜  \n",
            "4      ê°€ì•¼ì‚°ì²œë…„ìˆ˜  \n",
            "5       ì½”ì¹´í† ë ˆíƒ€  \n",
            "6  íŒ”ë„ë¹„ë½ì‹í˜œìº”xxx  \n",
            "7       ì¹ ì„±ì‚¬ì´ë‹¤  \n",
            "8        ì½”ì¹´ì½œë¼  \n",
            "9       ì½”ì¹´ì•”ë°”ì‚¬  \n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ ALL PREPROCESSING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Purchase Data: 28,295 rows\n",
            "ğŸ“Š Sales Data: 229,074 rows\n",
            "\n",
            "ğŸ’¾ Output files:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories_unique = a_purchase_processed['ì¤‘ë¶„ë¥˜'].unique()\n",
        "print(categories_unique)\n",
        "\n",
        "categories_unique = a_sales_processed['ì¤‘ë¶„ë¥˜'].unique()\n",
        "print(categories_unique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RedfRJBpWQ_y",
        "outputId": "c79a180a-beda-4e39-dc03-6ead098a1ba0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•' 'ì‹ ì„ ì‹í’ˆ']\n",
            "['ìƒìˆ˜,ìŒë£Œ,ê±´ê°•' 'ì‹ ì„ ì‹í’ˆ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a_purchase_processed.info())\n",
        "print('<=============================================>')\n",
        "print(a_sales_processed.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbLCIcIuWhOL",
        "outputId": "df3a70bb-36d8-45e1-bb85-e203d447cb1e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 28295 entries, 0 to 28294\n",
            "Data columns (total 8 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   ì¼ì         28295 non-null  object\n",
            " 1   ê³µê¸‰ì—…ì²´ ì½”ë“œ    28295 non-null  object\n",
            " 2   ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸  28295 non-null  object\n",
            " 3   ìƒí’ˆëª…        28295 non-null  object\n",
            " 4   EA         28295 non-null  object\n",
            " 5   ì¤‘ë¶„ë¥˜        28295 non-null  object\n",
            " 6   ì†Œë¶„ë¥˜        28295 non-null  object\n",
            " 7   ë¶„ë¥˜í‚¤        28295 non-null  object\n",
            "dtypes: object(8)\n",
            "memory usage: 1.7+ MB\n",
            "None\n",
            "<=============================================>\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 229074 entries, 0 to 229073\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   íŒë§¤ì¼     229074 non-null  object\n",
            " 1   íŒë§¤ìˆ˜ëŸ‰    229074 non-null  object\n",
            " 2   ìƒí’ˆëª…     229074 non-null  object\n",
            " 3   ì¤‘ë¶„ë¥˜     229074 non-null  object\n",
            " 4   ì†Œë¶„ë¥˜     229074 non-null  object\n",
            " 5   ê³µê¸‰ê°€ì•¡    229074 non-null  object\n",
            " 6   ë¶€ê°€ì„¸     229074 non-null  object\n",
            " 7   ë¶„ë¥˜í‚¤     229074 non-null  object\n",
            "dtypes: object(8)\n",
            "memory usage: 14.0+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bë¬¼ë¥˜ì„¼í„° (purchase & sales)"
      ],
      "metadata": {
        "id": "YCMHb0e4YrO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bë¬¼ë¥˜ì„¼í„° Combined Purchase & Sales Data Preprocessing Pipeline\n",
        "# Filters by ì†Œë¶„ë¥˜ (íƒ„ì‚°ìŒë£Œ, ë´‰ì§€ë¼ë©´)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "from collections import Counter\n",
        "from difflib import get_close_matches\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: CLASSIFICATION KEY CREATION\n",
        "# ============================================================================\n",
        "\n",
        "def make_main_key(name):\n",
        "    \"\"\"\n",
        "    Creates a standardized classification key by cleaning product names.\n",
        "    Removes numbers, units, special characters, and brackets.\n",
        "    \"\"\"\n",
        "    if pd.isna(name):\n",
        "        return None\n",
        "\n",
        "    s = unicodedata.normalize(\"NFKC\", str(name)).lower()\n",
        "    # Remove content in brackets\n",
        "    s = re.sub(r\"[\\(\\[\\{<].*?[\\)\\]\\}>]\", \"\", s)\n",
        "    # Remove number patterns with units\n",
        "    s = re.sub(r\"\\b\\d+(?:\\.\\d+)?\\s*([xÃ—*]\\s*\\d+)\\s*(ea|cs|set|íŒ©|ê°œ|ì…)?\", \"\", s)\n",
        "    # Remove units\n",
        "    unit = r\"(ml|mL|l|â„“|g|kg|mg|oz|lb|mm|cm|m|ea|pcs?|pack|box|set|sheet|roll|í¬|ë´‰|ìº”|ë³‘|íŒ©|ê°œ|ì…|ë§¤|ë§¤ì…|ì„¸íŠ¸)\"\n",
        "    s = re.sub(rf\"\\b\\d+(?:\\.\\d+)?\\s*{unit}\\b\", \"\", s, flags=re.IGNORECASE)\n",
        "    # Remove remaining numbers\n",
        "    s = re.sub(r\"\\b\\d+(?:\\.\\d+)?\", \"\", s)\n",
        "    # Remove special characters\n",
        "    s = re.sub(r\"[\\[\\]<>ï¼œï¼/&+/Ã—*_\\-~Â·â€¢:;|,.!?\\^\\\"\\'\\`]\", \"\", s)\n",
        "    # Remove extra spaces\n",
        "    s = re.sub(r\"\\s+\", \"\", s).strip()\n",
        "\n",
        "    return s if s else None\n",
        "\n",
        "\n",
        "def split_container_key(df, key_col='ë¶„ë¥˜í‚¤'):\n",
        "    \"\"\"\n",
        "    Identifies container products and adds '_ìš©ê¸°' suffix to their classification key.\n",
        "    Uses strict patterns and food exclusion rules.\n",
        "    \"\"\"\n",
        "    # Strict container patterns\n",
        "    container_strict_pat = r\"(ë°€íìš©ê¸°|ë³´ê´€ìš©ê¸°|ë°˜ì°¬í†µ|ì°¬í•©|ë„ì‹œë½í†µ|ë½ì•¤ë½|íŠ¸ë¼ì´íƒ„|ë³´ê´€íƒ‘|ë³´ê´€ë³‘|ë³´ì˜¨ë„ì‹œë½|ë³´ì˜¨ë³‘|ë³´ëƒ‰ë°±|ì¿¨ëŸ¬ë°±|í‘¸ë“œì»¨í…Œì´ë„ˆ|ì§€í¼ë°±|ì§€í¼ë½|íŠ¸ë ˆì´|ë³´ê´€í•¨|ìœ ë¦¬ë°€í|ìŠ¤í…ë°€í|ìŠ¤í…Œì¸ë¦¬ìŠ¤ìš©ê¸°|ê¹€ì¹˜í†µ|ì–‘ë…í†µ|ë¶„ìœ ì»¨í…Œì´ë„ˆ|ìˆ˜ì €í†µ|ì–‘ë…ë³‘)\"\n",
        "\n",
        "    # Food patterns to exclude\n",
        "    food_pat = r\"(ê»Œ|ì ¤ë¦¬í†¨|ìº”ë””|ì‚¬íƒ•|ì´ˆì½œë¦¿|ì´ˆì½œ|ë¹„ìŠ¤í‚·|ì¿ í‚¤|ì ¤ë¦¬|ì¹´ë¼ë©œ|ì¸„ì‰|ì ¤|ìŠ¤ë‚µ|ë¼ë©´|ìš°ë™|êµ­ìˆ˜|ë©´|ê³¼ì|ìŒë£Œ|ì»¤í”¼|ì°¨|ì‹œë¦¬ì–¼|ëˆ„ë£½ì§€)\"\n",
        "\n",
        "    # Match strict container patterns\n",
        "    m_strict = df[\"ìƒí’ˆëª…\"].str.contains(container_strict_pat, regex=True, na=False)\n",
        "\n",
        "    # Match loose container pattern (contains \"ìš©ê¸°\") but not food\n",
        "    m_loose = df[\"ìƒí’ˆëª…\"].str.contains(r\"ìš©ê¸°\", regex=True, na=False) & ~df[\"ìƒí’ˆëª…\"].str.contains(food_pat, regex=True, na=False)\n",
        "\n",
        "    # Combine matches and exclude food\n",
        "    mask_container = (m_strict | m_loose) & ~df[\"ìƒí’ˆëª…\"].str.contains(food_pat, regex=True, na=False)\n",
        "\n",
        "    # Add '_ìš©ê¸°' suffix\n",
        "    df.loc[mask_container, key_col] = df.loc[mask_container, key_col] + \"_ìš©ê¸°\"\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: AUTOMATED CLASSIFICATION FILLING\n",
        "# ============================================================================\n",
        "\n",
        "def fill_main_by_key(df, key_col, label_col, min_support=3, purity=0.9,\n",
        "                     restrict_to_existing=True, fallback_value=None):\n",
        "    \"\"\"\n",
        "    Automatically fills missing classifications based on key patterns.\n",
        "    Uses statistical validation with minimum support and purity thresholds.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "\n",
        "    # Calculate frequency of (key, label) pairs\n",
        "    g = (out.dropna(subset=[key_col, label_col])\n",
        "            .groupby([key_col, label_col]).size()\n",
        "            .rename('cnt').reset_index())\n",
        "\n",
        "    if g.empty:\n",
        "        return out, {}, pd.DataFrame()\n",
        "\n",
        "    # Calculate total count per key and most frequent label\n",
        "    tot = g.groupby(key_col)['cnt'].sum().rename('tot')\n",
        "    top = (g.sort_values(['cnt'], ascending=False)\n",
        "                .groupby(key_col)\n",
        "                .head(1)\n",
        "                .set_index(key_col))\n",
        "\n",
        "    stat = top.join(tot).reset_index()\n",
        "    stat['purity'] = stat['cnt'] / stat['tot']\n",
        "\n",
        "    # Filter by minimum support and purity\n",
        "    ok = stat[(stat['tot'] >= min_support) & (stat['purity'] >= purity)]\n",
        "    key2label = dict(zip(ok[key_col], ok[label_col]))\n",
        "\n",
        "    # Restrict to existing classification values\n",
        "    if restrict_to_existing:\n",
        "        candidates = df[label_col].dropna().unique().tolist()\n",
        "        new_key2label = {}\n",
        "\n",
        "        for k, v in key2label.items():\n",
        "            if v in candidates:\n",
        "                new_key2label[k] = v\n",
        "            else:\n",
        "                # Try fuzzy matching\n",
        "                match = get_close_matches(v, candidates, n=1, cutoff=0.6)\n",
        "                if match:\n",
        "                    new_key2label[k] = match[0]\n",
        "                elif fallback_value is not None:\n",
        "                    new_key2label[k] = fallback_value\n",
        "\n",
        "        key2label = new_key2label\n",
        "\n",
        "    # Fill missing values\n",
        "    mask = out[label_col].isna()\n",
        "    out.loc[mask, label_col] = out.loc[mask, key_col].map(key2label)\n",
        "\n",
        "    # Create statistics report\n",
        "    report = (stat.assign(adopted=stat[key_col].isin(ok[key_col]))\n",
        "                    .sort_values(['adopted', 'purity', 'tot'],\n",
        "                                ascending=[True, False, False]))\n",
        "\n",
        "    return out, key2label, report\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: BRAND PREFIX ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_brand_prefixes(df, subcol='ì†Œë¶„ë¥˜'):\n",
        "    \"\"\"\n",
        "    Analyzes brand prefixes (text before ']') in product names.\n",
        "    Returns statistics on how prefixes map to subcategories.\n",
        "    \"\"\"\n",
        "    pat = re.compile(r'^\\s*([^]]+)]')\n",
        "\n",
        "    # Extract prefixes from non-null subcategories\n",
        "    with_sub = df[df[subcol].notna()].copy()\n",
        "    prefixes = set()\n",
        "\n",
        "    for name in with_sub['ìƒí’ˆëª…'].dropna():\n",
        "        m = pat.match(name)\n",
        "        if m:\n",
        "            prefixes.add(m.group(1).strip())\n",
        "\n",
        "    # Analyze distribution\n",
        "    stats = []\n",
        "    for prefix in sorted(prefixes):\n",
        "        regex = re.compile(rf'(^|\\s){re.escape(prefix)}\\]')\n",
        "        matched = with_sub[with_sub['ìƒí’ˆëª…'].str.contains(regex, na=False)]\n",
        "\n",
        "        if not matched.empty:\n",
        "            counts = matched[subcol].value_counts()\n",
        "            for cat, cnt in counts.items():\n",
        "                stats.append({\n",
        "                    'ë¸Œëœë“œ': prefix,\n",
        "                    'ì†Œë¶„ë¥˜': cat,\n",
        "                    'ìƒí’ˆëª…_ê°œìˆ˜': cnt\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(stats).sort_values(['ë¸Œëœë“œ', 'ìƒí’ˆëª…_ê°œìˆ˜'],\n",
        "                                          ascending=[True, False])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: SALES DATA HARMONIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def harmonize_sales_columns(df, dataset_type='2123'):\n",
        "    \"\"\"\n",
        "    Harmonizes column names between 2021-23 and 2024 sales datasets.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Sales dataframe to harmonize\n",
        "    dataset_type : str\n",
        "        '2123' for 2021-23 data, '24' for 2024 data\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Print current columns for debugging\n",
        "    print(f\"   Columns before harmonization ({dataset_type}):\")\n",
        "    print(f\"   {list(df.columns)}\")\n",
        "\n",
        "    if dataset_type == '2123':\n",
        "        # Rename columns to match standard naming\n",
        "        rename_map = {\n",
        "            'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸': 'ìš°í¸ë²ˆí˜¸',\n",
        "            'ë°”ì½”ë“œ': 'ìƒí’ˆë°”ì½”ë“œ',\n",
        "            'ê³µê¸‰ê¸ˆì•¡': 'ê³µê¸‰ê°€ì•¡',\n",
        "            'ë¶€ê°€ì„¸(ê³¼ì„¸)': 'ë¶€ê°€ì„¸',\n",
        "            'ì˜µì…˜ ì½”ë“œ': 'ì˜µì…˜ì½”ë“œ'\n",
        "        }\n",
        "\n",
        "        # Check which columns actually exist\n",
        "        existing_renames = {k: v for k, v in rename_map.items() if k in df.columns}\n",
        "        print(f\"   Renaming: {existing_renames}\")\n",
        "        df = df.rename(columns=existing_renames)\n",
        "\n",
        "    elif dataset_type == '24':\n",
        "        # Rename columns to match standard naming\n",
        "        rename_map = {\n",
        "            'ìƒí’ˆ ë°”ì½”ë“œ(ëŒ€í•œìƒì˜)': 'ìƒí’ˆë°”ì½”ë“œ',\n",
        "            'ì˜µì…˜ì½”ë“œ': 'ì˜µì…˜ì½”ë“œ'  # Ensure consistency\n",
        "        }\n",
        "\n",
        "        # Check which columns actually exist\n",
        "        existing_renames = {k: v for k, v in rename_map.items() if k in df.columns}\n",
        "        print(f\"   Renaming: {existing_renames}\")\n",
        "        df = df.rename(columns=existing_renames)\n",
        "\n",
        "    print(f\"   Columns after harmonization:\")\n",
        "    print(f\"   {list(df.columns)}\\n\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_and_merge_sales_data(filepath_2123, filepath_24):\n",
        "    \"\"\"\n",
        "    Loads both sales datasets and merges them into a single dataframe.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    filepath_2123 : str\n",
        "        Path to 2021-23 sales data CSV\n",
        "    filepath_24 : str\n",
        "        Path to 2024 sales data CSV\n",
        "    \"\"\"\n",
        "    print(\"\\n[Loading Sales Data]\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Load 2021-23 data\n",
        "    print(f\"Loading {filepath_2123}...\")\n",
        "    df_2123 = pd.read_csv(filepath_2123, dtype=str)\n",
        "    print(f\"   Rows: {len(df_2123):,}\")\n",
        "    df_2123 = harmonize_sales_columns(df_2123, dataset_type='2123')\n",
        "\n",
        "    # Load 2024 data\n",
        "    print(f\"Loading {filepath_24}...\")\n",
        "    df_24 = pd.read_csv(filepath_24, dtype=str)\n",
        "    print(f\"   Rows: {len(df_24):,}\")\n",
        "    df_24 = harmonize_sales_columns(df_24, dataset_type='24')\n",
        "\n",
        "    # Merge datasets\n",
        "    print(\"Merging datasets...\")\n",
        "    df_merged = pd.concat([df_2123, df_24], ignore_index=True)\n",
        "    print(f\"   Total rows after merge: {len(df_merged):,}\")\n",
        "\n",
        "    # Consolidate duplicate columns (in case renaming didn't work perfectly)\n",
        "    print(\"\\nConsolidating duplicate columns...\")\n",
        "    duplicate_pairs = [\n",
        "        ('ê³µê¸‰ê°€ì•¡', 'ê³µê¸‰ê¸ˆì•¡'),\n",
        "        ('ë¶€ê°€ì„¸', 'ë¶€ê°€ì„¸(ê³¼ì„¸)'),\n",
        "        ('ìš°í¸ë²ˆí˜¸', 'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸'),\n",
        "        ('ìƒí’ˆë°”ì½”ë“œ', 'ë°”ì½”ë“œ'),\n",
        "        ('ì˜µì…˜ì½”ë“œ', 'ì˜µì…˜ ì½”ë“œ')\n",
        "    ]\n",
        "\n",
        "    for primary, secondary in duplicate_pairs:\n",
        "        if primary in df_merged.columns and secondary in df_merged.columns:\n",
        "            print(f\"   Merging {primary} â† {secondary}\")\n",
        "            # Fill primary column with secondary where primary is null\n",
        "            df_merged[primary] = df_merged[primary].fillna(df_merged[secondary])\n",
        "            # Drop the secondary column\n",
        "            df_merged = df_merged.drop(columns=[secondary])\n",
        "        elif secondary in df_merged.columns and primary not in df_merged.columns:\n",
        "            print(f\"   Renaming {secondary} â†’ {primary}\")\n",
        "            df_merged = df_merged.rename(columns={secondary: primary})\n",
        "\n",
        "    print(f\"\\n   Final columns after consolidation:\")\n",
        "    print(f\"   {list(df_merged.columns)}\")\n",
        "    print(f\"   Non-null counts:\")\n",
        "    for col in df_merged.columns:\n",
        "        print(f\"      {col}: {df_merged[col].notna().sum():,}\")\n",
        "\n",
        "    return df_merged\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 5: MAIN PREPROCESSING PIPELINES\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_purchase_data(filepath, target_categories=None):\n",
        "    \"\"\"\n",
        "    Main preprocessing pipeline for Bë¬¼ë¥˜ì„¼í„° purchase data.\n",
        "    Filters by ì†Œë¶„ë¥˜ (not ì¤‘ë¶„ë¥˜ like Aë¬¼ë¥˜ì„¼í„°).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    filepath : str\n",
        "        Path to the purchase data CSV file\n",
        "    target_categories : list, optional\n",
        "        List of target ì†Œë¶„ë¥˜ categories to filter\n",
        "        Default: ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\n",
        "    \"\"\"\n",
        "\n",
        "    if target_categories is None:\n",
        "        target_categories = ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Bë¬¼ë¥˜ì„¼í„° PURCHASE DATA PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Load data\n",
        "    print(\"\\n[Step 1] Loading data...\")\n",
        "    df = pd.read_csv(filepath, dtype=str)\n",
        "    print(f\"   Initial rows: {len(df):,}\")\n",
        "\n",
        "    # Step 2: Remove returns\n",
        "    print(\"\\n[Step 2] Removing return entries (ë°˜ì¶œ)...\")\n",
        "    df = df[df['ì‘ì—…ìœ í˜•'] != 'ë°˜ì¶œ'].copy()\n",
        "    print(f\"   Rows after removal: {len(df):,}\")\n",
        "\n",
        "    # Step 3: Create classification key\n",
        "    print(\"\\n[Step 3] Creating classification keys...\")\n",
        "    df['ë¶„ë¥˜í‚¤'] = df['ìƒí’ˆëª…'].apply(make_main_key)\n",
        "    df = split_container_key(df, key_col='ë¶„ë¥˜í‚¤')\n",
        "    print(f\"   Unique keys created: {df['ë¶„ë¥˜í‚¤'].nunique():,}\")\n",
        "\n",
        "    # Step 4: Fill missing classifications (initial pass - groupby)\n",
        "    print(\"\\n[Step 4] Initial classification filling (groupby method)...\")\n",
        "    for col in [\"ëŒ€ë¶„ë¥˜\", \"ì¤‘ë¶„ë¥˜\", \"ì†Œë¶„ë¥˜\"]:\n",
        "        before = df[col].isna().sum()\n",
        "        mask = df[col].isna()\n",
        "        df.loc[mask, col] = (\n",
        "            df.groupby(\"ë¶„ë¥˜í‚¤\")[col]\n",
        "            .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "        )[mask]\n",
        "        after = df[col].isna().sum()\n",
        "        print(f\"   {col}: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 5: Fill missing barcodes\n",
        "    print(\"\\n[Step 5] Filling missing barcodes...\")\n",
        "    if \"ë°”ì½”ë“œ\" in df.columns:\n",
        "        before = df[\"ë°”ì½”ë“œ\"].isna().sum()\n",
        "        mask = df[\"ë°”ì½”ë“œ\"].isna()\n",
        "        if \"ìƒí’ˆì½”ë“œ\" in df.columns:\n",
        "            df.loc[mask, \"ë°”ì½”ë“œ\"] = (\n",
        "                df.groupby(\"ìƒí’ˆì½”ë“œ\")[\"ë°”ì½”ë“œ\"]\n",
        "                .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "            )[mask]\n",
        "        after = df[\"ë°”ì½”ë“œ\"].isna().sum()\n",
        "        print(f\"   Barcodes: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 6: Filter by target categories (ì†Œë¶„ë¥˜ for Bë¬¼ë¥˜ì„¼í„°)\n",
        "    print(f\"\\n[Step 6] Filtering by target ì†Œë¶„ë¥˜: {target_categories}...\")\n",
        "    df_filtered = df[df['ì†Œë¶„ë¥˜'].isin(target_categories)].copy()\n",
        "    print(f\"   Rows after filtering: {len(df_filtered):,}\")\n",
        "    print(f\"   Unique products: {df_filtered['ìƒí’ˆëª…'].nunique():,}\")\n",
        "\n",
        "    # Step 7: Automated classification filling (advanced)\n",
        "    print(\"\\n[Step 7] Advanced classification filling...\")\n",
        "    df_updated = df_filtered.copy()\n",
        "\n",
        "    # Fill ëŒ€ë¶„ë¥˜\n",
        "    before = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    df_filled, key2label, report = fill_main_by_key(\n",
        "        df_updated,\n",
        "        key_col='ë¶„ë¥˜í‚¤',\n",
        "        label_col='ëŒ€ë¶„ë¥˜',\n",
        "        min_support=2,\n",
        "        purity=0.7\n",
        "    )\n",
        "\n",
        "    fill_map = df_filled.dropna(subset=['ëŒ€ë¶„ë¥˜']).groupby('ë¶„ë¥˜í‚¤')['ëŒ€ë¶„ë¥˜'].first()\n",
        "    mask = df_updated['ëŒ€ë¶„ë¥˜'].isna()\n",
        "    df_updated.loc[mask, 'ëŒ€ë¶„ë¥˜'] = df_updated.loc[mask, 'ë¶„ë¥˜í‚¤'].map(fill_map)\n",
        "    after = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    print(f\"   ëŒ€ë¶„ë¥˜: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 8: Clean up columns\n",
        "    print(\"\\n[Step 8] Cleaning up columns...\")\n",
        "    columns_to_drop = [\n",
        "        'ì‘ì—…ìœ í˜•', 'ë§¤ì¶œì²˜ì½”ë“œ', 'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸', 'ì…ê³  í˜•íƒœ',\n",
        "        'ë°”ì½”ë“œ', 'ê·œê²©', 'ì…ìˆ˜', 'ìˆ˜ëŸ‰', 'íŒë§¤ê¸ˆì•¡', 'ë¶€ê°€ì„¸(ê³¼ì„¸)',\n",
        "        'ìƒí’ˆì½”ë“œ', 'ì˜µì…˜ ì½”ë“œ', 'ì˜µì…˜', 'ëŒ€ë¶„ë¥˜'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_updated.columns]\n",
        "    df_updated = df_updated.drop(columns_to_drop, axis=1)\n",
        "    print(f\"   Dropped {len(columns_to_drop)} columns\")\n",
        "    print(f\"   Remaining columns: {list(df_updated.columns)}\")\n",
        "\n",
        "    # Step 9: Final cleanup\n",
        "    print(\"\\n[Step 9] Final cleanup...\")\n",
        "    df_updated = df_updated.reset_index(drop=True)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPROCESSING COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nFinal dataset shape: {df_updated.shape}\")\n",
        "    print(f\"Rows: {len(df_updated):,}\")\n",
        "    print(f\"Columns: {len(df_updated.columns)}\")\n",
        "    print(f\"\\nMissing values per column:\")\n",
        "    print(df_updated.isnull().sum())\n",
        "    print(f\"\\nUnique values per classification:\")\n",
        "    print(f\"  ì¤‘ë¶„ë¥˜: {df_updated['ì¤‘ë¶„ë¥˜'].nunique()}\")\n",
        "    print(f\"  ì†Œë¶„ë¥˜: {df_updated['ì†Œë¶„ë¥˜'].nunique()}\")\n",
        "\n",
        "    return df_updated\n",
        "\n",
        "\n",
        "def preprocess_sales_data(filepath_2123, filepath_24, target_categories=None):\n",
        "    \"\"\"\n",
        "    Main preprocessing pipeline for Bë¬¼ë¥˜ì„¼í„° sales data.\n",
        "    Filters by ì†Œë¶„ë¥˜ (not ì¤‘ë¶„ë¥˜ like Aë¬¼ë¥˜ì„¼í„°).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    filepath_2123 : str\n",
        "        Path to the 2021-23 sales data CSV file\n",
        "    filepath_24 : str\n",
        "        Path to the 2024 sales data CSV file\n",
        "    target_categories : list, optional\n",
        "        List of target ì†Œë¶„ë¥˜ categories to filter\n",
        "        Default: ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\n",
        "    \"\"\"\n",
        "\n",
        "    if target_categories is None:\n",
        "        target_categories = ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Bë¬¼ë¥˜ì„¼í„° SALES DATA PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Load and merge data\n",
        "    df = load_and_merge_sales_data(filepath_2123, filepath_24)\n",
        "\n",
        "    # Step 2: Create classification key\n",
        "    print(\"\\n[Step 2] Creating classification keys...\")\n",
        "    df['ë¶„ë¥˜í‚¤'] = df['ìƒí’ˆëª…'].apply(make_main_key)\n",
        "    df = split_container_key(df, key_col='ë¶„ë¥˜í‚¤')\n",
        "    print(f\"   Unique keys created: {df['ë¶„ë¥˜í‚¤'].nunique():,}\")\n",
        "\n",
        "    # Step 3: Fill missing classifications (initial pass - groupby)\n",
        "    print(\"\\n[Step 3] Initial classification filling (groupby method)...\")\n",
        "    for col in [\"ëŒ€ë¶„ë¥˜\", \"ì¤‘ë¶„ë¥˜\", \"ì†Œë¶„ë¥˜\"]:\n",
        "        before = df[col].isna().sum()\n",
        "        mask = df[col].isna()\n",
        "        df.loc[mask, col] = (\n",
        "            df.groupby(\"ë¶„ë¥˜í‚¤\")[col]\n",
        "            .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "        )[mask]\n",
        "        after = df[col].isna().sum()\n",
        "        print(f\"   {col}: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 4: Fill missing barcodes\n",
        "    print(\"\\n[Step 4] Filling missing barcodes...\")\n",
        "    if \"ìƒí’ˆë°”ì½”ë“œ\" in df.columns:\n",
        "        before = df[\"ìƒí’ˆë°”ì½”ë“œ\"].isna().sum()\n",
        "        mask = df[\"ìƒí’ˆë°”ì½”ë“œ\"].isna()\n",
        "        df.loc[mask, \"ìƒí’ˆë°”ì½”ë“œ\"] = (\n",
        "            df.groupby(\"ìƒí’ˆëª…\")[\"ìƒí’ˆë°”ì½”ë“œ\"]\n",
        "            .transform(lambda x: x.fillna(x.dropna().iloc[0]) if x.dropna().size > 0 else x)\n",
        "        )[mask]\n",
        "        after = df[\"ìƒí’ˆë°”ì½”ë“œ\"].isna().sum()\n",
        "        print(f\"   Barcodes: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 5: Filter by target categories (ì†Œë¶„ë¥˜ for Bë¬¼ë¥˜ì„¼í„°)\n",
        "    print(f\"\\n[Step 5] Filtering by target ì†Œë¶„ë¥˜: {target_categories}...\")\n",
        "    df_filtered = df[df['ì†Œë¶„ë¥˜'].isin(target_categories)].copy()\n",
        "    print(f\"   Rows after filtering: {len(df_filtered):,}\")\n",
        "    print(f\"   Unique products: {df_filtered['ìƒí’ˆëª…'].nunique():,}\")\n",
        "\n",
        "    # Step 6: Automated classification filling (advanced)\n",
        "    print(\"\\n[Step 6] Advanced classification filling...\")\n",
        "    df_updated = df_filtered.copy()\n",
        "\n",
        "    # Fill ëŒ€ë¶„ë¥˜\n",
        "    before = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    df_filled, key2label, report = fill_main_by_key(\n",
        "        df_updated,\n",
        "        key_col='ë¶„ë¥˜í‚¤',\n",
        "        label_col='ëŒ€ë¶„ë¥˜',\n",
        "        min_support=2,\n",
        "        purity=0.7\n",
        "    )\n",
        "\n",
        "    fill_map = df_filled.dropna(subset=['ëŒ€ë¶„ë¥˜']).groupby('ë¶„ë¥˜í‚¤')['ëŒ€ë¶„ë¥˜'].first()\n",
        "    mask = df_updated['ëŒ€ë¶„ë¥˜'].isna()\n",
        "    df_updated.loc[mask, 'ëŒ€ë¶„ë¥˜'] = df_updated.loc[mask, 'ë¶„ë¥˜í‚¤'].map(fill_map)\n",
        "    after = df_updated['ëŒ€ë¶„ë¥˜'].isna().sum()\n",
        "    print(f\"   ëŒ€ë¶„ë¥˜: {before:,} â†’ {after:,} missing ({before-after:,} filled)\")\n",
        "\n",
        "    # Step 7: Clean up columns\n",
        "    print(\"\\n[Step 7] Cleaning up columns...\")\n",
        "    columns_to_drop = [\n",
        "        'êµ¬ë¶„', 'ë§¤ì¶œì²˜ì½”ë“œ', 'ìš°í¸ë²ˆí˜¸', 'ì˜µì…˜ì½”ë“œ',\n",
        "        'ê·œê²©', 'ì…ìˆ˜', 'ìƒí’ˆë°”ì½”ë“œ', 'ëŒ€ë¶„ë¥˜'\n",
        "    ]\n",
        "\n",
        "    # Only drop columns that exist\n",
        "    columns_to_drop = [col for col in columns_to_drop if col in df_updated.columns]\n",
        "    df_updated = df_updated.drop(columns_to_drop, axis=1)\n",
        "    print(f\"   Dropped {len(columns_to_drop)} columns\")\n",
        "    print(f\"   Remaining columns: {list(df_updated.columns)}\")\n",
        "\n",
        "    # Step 8: Final cleanup\n",
        "    print(\"\\n[Step 8] Final cleanup...\")\n",
        "    df_updated = df_updated.reset_index(drop=True)\n",
        "\n",
        "    # Summary statistics\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PREPROCESSING COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nFinal dataset shape: {df_updated.shape}\")\n",
        "    print(f\"Rows: {len(df_updated):,}\")\n",
        "    print(f\"Columns: {len(df_updated.columns)}\")\n",
        "    print(f\"\\nMissing values per column:\")\n",
        "    print(df_updated.isnull().sum())\n",
        "    print(f\"\\nUnique values per classification:\")\n",
        "    print(f\"  ì¤‘ë¶„ë¥˜: {df_updated['ì¤‘ë¶„ë¥˜'].nunique()}\")\n",
        "    print(f\"  ì†Œë¶„ë¥˜: {df_updated['ì†Œë¶„ë¥˜'].nunique()}\")\n",
        "\n",
        "    return df_updated\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 6: USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # ========================================================================\n",
        "    # Process PURCHASE data\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"ğŸ”¹\" * 40)\n",
        "    print(\"PROCESSING Bë¬¼ë¥˜ì„¼í„° PURCHASE DATA\")\n",
        "    print(\"ğŸ”¹\" * 40)\n",
        "\n",
        "    b_purchase_processed = preprocess_purchase_data(\n",
        "        'B_purchase_data.csv',\n",
        "        target_categories=['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\n",
        "    )\n",
        "\n",
        "    # Save processed purchase data\n",
        "    # purchase_output = 'B_purchase_data_processed.csv'\n",
        "    # b_purchase_processed.to_csv(purchase_output, index=False, encoding='utf-8-sig')\n",
        "    # print(f\"\\nâœ… Processed purchase data saved to: {purchase_output}\")\n",
        "\n",
        "    print(\"\\nSample of processed purchase data:\")\n",
        "    print(b_purchase_processed.head(10))\n",
        "\n",
        "    # ========================================================================\n",
        "    # Process SALES data\n",
        "    # ========================================================================\n",
        "    print(\"\\n\\n\" + \"ğŸ”¹\" * 40)\n",
        "    print(\"PROCESSING Bë¬¼ë¥˜ì„¼í„° SALES DATA\")\n",
        "    print(\"ğŸ”¹\" * 40)\n",
        "\n",
        "    b_sales_processed = preprocess_sales_data(\n",
        "        'b_sales_2123.csv',\n",
        "        'b_sales_24.csv',\n",
        "        target_categories=['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\n",
        "    )\n",
        "\n",
        "    # Save processed sales data\n",
        "    # sales_output = 'B_sales_data_processed.csv'\n",
        "    # b_sales_processed.to_csv(sales_output, index=False, encoding='utf-8-sig')\n",
        "    # print(f\"\\nâœ… Processed sales data saved to: {sales_output}\")\n",
        "\n",
        "    print(\"\\nSample of processed sales data:\")\n",
        "    print(b_sales_processed.head(10))\n",
        "\n",
        "    # ========================================================================\n",
        "    # Final Summary\n",
        "    # ========================================================================\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"ğŸ‰ ALL Bë¬¼ë¥˜ì„¼í„° PREPROCESSING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nğŸ“Š Purchase Data: {len(b_purchase_processed):,} rows\")\n",
        "    print(f\"ğŸ“Š Sales Data: {len(b_sales_processed):,} rows\")\n",
        "    print(f\"\\nğŸ’¾ Output files:\")\n",
        "    # print(f\"   - {purchase_output}\")\n",
        "    # print(f\"   - {sales_output}\")\n",
        "    print(f\"\\nğŸ¯ Filtered by ì†Œë¶„ë¥˜: ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go9ElO_jYvmd",
        "outputId": "f81467ef-6a7b-4f5f-8668-489e0fa80bbf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "PROCESSING Bë¬¼ë¥˜ì„¼í„° PURCHASE DATA\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "================================================================================\n",
            "Bë¬¼ë¥˜ì„¼í„° PURCHASE DATA PREPROCESSING PIPELINE\n",
            "================================================================================\n",
            "\n",
            "[Step 1] Loading data...\n",
            "   Initial rows: 345,021\n",
            "\n",
            "[Step 2] Removing return entries (ë°˜ì¶œ)...\n",
            "   Rows after removal: 303,107\n",
            "\n",
            "[Step 3] Creating classification keys...\n",
            "   Unique keys created: 9,505\n",
            "\n",
            "[Step 4] Initial classification filling (groupby method)...\n",
            "   ëŒ€ë¶„ë¥˜: 5 â†’ 2 missing (3 filled)\n",
            "   ì¤‘ë¶„ë¥˜: 202,166 â†’ 199,896 missing (2,270 filled)\n",
            "   ì†Œë¶„ë¥˜: 205,486 â†’ 203,531 missing (1,955 filled)\n",
            "\n",
            "[Step 5] Filling missing barcodes...\n",
            "   Barcodes: 72 â†’ 64 missing (8 filled)\n",
            "\n",
            "[Step 6] Filtering by target ì†Œë¶„ë¥˜: ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']...\n",
            "   Rows after filtering: 4,367\n",
            "   Unique products: 202\n",
            "\n",
            "[Step 7] Advanced classification filling...\n",
            "   ëŒ€ë¶„ë¥˜: 0 â†’ 0 missing (0 filled)\n",
            "\n",
            "[Step 8] Cleaning up columns...\n",
            "   Dropped 14 columns\n",
            "   Remaining columns: ['ì¼ì', 'ê³µê¸‰ì—…ì²´ ì½”ë“œ', 'ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸', 'ìƒí’ˆëª…', 'EA', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ë¶„ë¥˜í‚¤']\n",
            "\n",
            "[Step 9] Final cleanup...\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Final dataset shape: (4367, 8)\n",
            "Rows: 4,367\n",
            "Columns: 8\n",
            "\n",
            "Missing values per column:\n",
            "ì¼ì           0\n",
            "ê³µê¸‰ì—…ì²´ ì½”ë“œ      0\n",
            "ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸    0\n",
            "ìƒí’ˆëª…          0\n",
            "EA           0\n",
            "ì¤‘ë¶„ë¥˜          0\n",
            "ì†Œë¶„ë¥˜          0\n",
            "ë¶„ë¥˜í‚¤          0\n",
            "dtype: int64\n",
            "\n",
            "Unique values per classification:\n",
            "  ì¤‘ë¶„ë¥˜: 2\n",
            "  ì†Œë¶„ë¥˜: 2\n",
            "\n",
            "Sample of processed purchase data:\n",
            "           ì¼ì ê³µê¸‰ì—…ì²´ ì½”ë“œ ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸                   ìƒí’ˆëª…    EA  ì¤‘ë¶„ë¥˜   ì†Œë¶„ë¥˜  \\\n",
            "0  2021-01-05  300008     47026       (ë†ì‹¬)ì‹ ë¼ë©´ë©€í‹°120g*5  1344  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   \n",
            "1  2021-01-05  100004     51402      (ë¡¯ë°ì¹ ì„±)íŠ¸ë ˆë¹„ê¸ˆê·¤500ml   320   ìŒë£Œ  íƒ„ì‚°ìŒë£Œ   \n",
            "2  2021-01-05  100004     51402      (ë¡¯ë°ì¹ ì„±)ì¹ ì„±ì‚¬ì´ë‹¤250ml  4500   ìŒë£Œ  íƒ„ì‚°ìŒë£Œ   \n",
            "3  2021-01-05  100108      7270      (ë†ì‹¬)ë©¸ì¹˜ì¹¼êµ­ìˆ˜ë©€í‹°98g*5    80  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   \n",
            "4  2021-01-05  100108      7270     (ë†ì‹¬)ì‹ ë¼ë©´ë¸”ë™ë©€í‹°134g*4    80  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   \n",
            "5  2021-01-05  100010     50859       (ì½”ì¹´ì½œë¼)ì½”ì¹´ì½œë¼500ml  1152   ìŒë£Œ  íƒ„ì‚°ìŒë£Œ   \n",
            "6  2021-01-05  100010     50859   (ì½”ì¹´ì½œë¼)ë°€í¬ì†Œë‹¤1.5L(ì•”ë°”ì‚¬)   132   ìŒë£Œ  íƒ„ì‚°ìŒë£Œ   \n",
            "7  2021-01-05  300023     41142       (ë†ì‹¬)ì›°ì¹˜ìŠ¤ì†Œë‹¤í¬ë„1.5L   120   ìŒë£Œ  íƒ„ì‚°ìŒë£Œ   \n",
            "8  2021-01-05  300023     41142  (ì½”ì¹´ì½œë¼)ì½”ì¹´ì½œë¼(ìŠˆí¼ìš©)355ml   480   ìŒë£Œ  íƒ„ì‚°ìŒë£Œ   \n",
            "9  2021-01-05  300023     41142       (ì½”ì¹´ì½œë¼)ì½”ì¹´ì½œë¼190ml   720   ìŒë£Œ  íƒ„ì‚°ìŒë£Œ   \n",
            "\n",
            "           ë¶„ë¥˜í‚¤  \n",
            "0    ì‹ ë¼ë©´ë©€í‹°120g  \n",
            "1   íŠ¸ë ˆë¹„ê¸ˆê·¤500ml  \n",
            "2   ì¹ ì„±ì‚¬ì´ë‹¤250ml  \n",
            "3   ë©¸ì¹˜ì¹¼êµ­ìˆ˜ë©€í‹°98g  \n",
            "4  ì‹ ë¼ë©´ë¸”ë™ë©€í‹°134g  \n",
            "5    ì½”ì¹´ì½œë¼500ml  \n",
            "6        ë°€í¬ì†Œë‹¤1  \n",
            "7     ì›°ì¹˜ìŠ¤ì†Œë‹¤í¬ë„1  \n",
            "8    ì½”ì¹´ì½œë¼355ml  \n",
            "9    ì½”ì¹´ì½œë¼190ml  \n",
            "\n",
            "\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "PROCESSING Bë¬¼ë¥˜ì„¼í„° SALES DATA\n",
            "ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹ğŸ”¹\n",
            "================================================================================\n",
            "Bë¬¼ë¥˜ì„¼í„° SALES DATA PREPROCESSING PIPELINE\n",
            "================================================================================\n",
            "\n",
            "[Loading Sales Data]\n",
            "--------------------------------------------------------------------------------\n",
            "Loading b_sales_2123.csv...\n",
            "   Rows: 1,048,575\n",
            "   Columns before harmonization (2123):\n",
            "   ['íŒë§¤ì¼', 'êµ¬ë¶„', 'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸', 'ë§¤ì¶œì²˜ì½”ë“œ', 'íŒë§¤ìˆ˜ëŸ‰', 'ì˜µì…˜ ì½”ë“œ', 'ê·œê²©', 'ì…ìˆ˜', 'ë°”ì½”ë“œ', 'ìƒí’ˆëª…', 'ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ê³µê¸‰ê¸ˆì•¡', 'ë¶€ê°€ì„¸(ê³¼ì„¸)']\n",
            "   Renaming: {'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸': 'ìš°í¸ë²ˆí˜¸', 'ë°”ì½”ë“œ': 'ìƒí’ˆë°”ì½”ë“œ', 'ê³µê¸‰ê¸ˆì•¡': 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸(ê³¼ì„¸)': 'ë¶€ê°€ì„¸', 'ì˜µì…˜ ì½”ë“œ': 'ì˜µì…˜ì½”ë“œ'}\n",
            "   Columns after harmonization:\n",
            "   ['íŒë§¤ì¼', 'êµ¬ë¶„', 'ìš°í¸ë²ˆí˜¸', 'ë§¤ì¶œì²˜ì½”ë“œ', 'íŒë§¤ìˆ˜ëŸ‰', 'ì˜µì…˜ì½”ë“œ', 'ê·œê²©', 'ì…ìˆ˜', 'ìƒí’ˆë°”ì½”ë“œ', 'ìƒí’ˆëª…', 'ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸']\n",
            "\n",
            "Loading b_sales_24.csv...\n",
            "   Rows: 493,327\n",
            "   Columns before harmonization (24):\n",
            "   ['íŒë§¤ì¼', 'êµ¬ë¶„', 'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸', 'ë§¤ì¶œì²˜ì½”ë“œ', 'íŒë§¤ìˆ˜ëŸ‰', 'ì˜µì…˜ ì½”ë“œ', 'ê·œê²©', 'ì…ìˆ˜', 'ë°”ì½”ë“œ', 'ìƒí’ˆëª…', 'ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ê³µê¸‰ê¸ˆì•¡', 'ë¶€ê°€ì„¸(ê³¼ì„¸)']\n",
            "   Renaming: {}\n",
            "   Columns after harmonization:\n",
            "   ['íŒë§¤ì¼', 'êµ¬ë¶„', 'ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸', 'ë§¤ì¶œì²˜ì½”ë“œ', 'íŒë§¤ìˆ˜ëŸ‰', 'ì˜µì…˜ ì½”ë“œ', 'ê·œê²©', 'ì…ìˆ˜', 'ë°”ì½”ë“œ', 'ìƒí’ˆëª…', 'ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ê³µê¸‰ê¸ˆì•¡', 'ë¶€ê°€ì„¸(ê³¼ì„¸)']\n",
            "\n",
            "Merging datasets...\n",
            "   Total rows after merge: 1,541,902\n",
            "\n",
            "Consolidating duplicate columns...\n",
            "   Merging ê³µê¸‰ê°€ì•¡ â† ê³µê¸‰ê¸ˆì•¡\n",
            "   Merging ë¶€ê°€ì„¸ â† ë¶€ê°€ì„¸(ê³¼ì„¸)\n",
            "   Merging ìš°í¸ë²ˆí˜¸ â† ë§¤ì¶œì²˜ ìš°í¸ë²ˆí˜¸\n",
            "   Merging ìƒí’ˆë°”ì½”ë“œ â† ë°”ì½”ë“œ\n",
            "   Merging ì˜µì…˜ì½”ë“œ â† ì˜µì…˜ ì½”ë“œ\n",
            "\n",
            "   Final columns after consolidation:\n",
            "   ['íŒë§¤ì¼', 'êµ¬ë¶„', 'ìš°í¸ë²ˆí˜¸', 'ë§¤ì¶œì²˜ì½”ë“œ', 'íŒë§¤ìˆ˜ëŸ‰', 'ì˜µì…˜ì½”ë“œ', 'ê·œê²©', 'ì…ìˆ˜', 'ìƒí’ˆë°”ì½”ë“œ', 'ìƒí’ˆëª…', 'ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸']\n",
            "   Non-null counts:\n",
            "      íŒë§¤ì¼: 1,541,902\n",
            "      êµ¬ë¶„: 1,541,902\n",
            "      ìš°í¸ë²ˆí˜¸: 1,541,902\n",
            "      ë§¤ì¶œì²˜ì½”ë“œ: 1,541,902\n",
            "      íŒë§¤ìˆ˜ëŸ‰: 1,541,902\n",
            "      ì˜µì…˜ì½”ë“œ: 1,541,902\n",
            "      ê·œê²©: 1,440,197\n",
            "      ì…ìˆ˜: 1,541,902\n",
            "      ìƒí’ˆë°”ì½”ë“œ: 1,541,731\n",
            "      ìƒí’ˆëª…: 1,541,902\n",
            "      ëŒ€ë¶„ë¥˜: 1,541,681\n",
            "      ì¤‘ë¶„ë¥˜: 1,201,628\n",
            "      ì†Œë¶„ë¥˜: 1,177,705\n",
            "      ê³µê¸‰ê°€ì•¡: 1,541,902\n",
            "      ë¶€ê°€ì„¸: 1,541,902\n",
            "\n",
            "[Step 2] Creating classification keys...\n",
            "   Unique keys created: 10,321\n",
            "\n",
            "[Step 3] Initial classification filling (groupby method)...\n",
            "   ëŒ€ë¶„ë¥˜: 221 â†’ 191 missing (30 filled)\n",
            "   ì¤‘ë¶„ë¥˜: 340,274 â†’ 332,974 missing (7,300 filled)\n",
            "   ì†Œë¶„ë¥˜: 364,197 â†’ 355,609 missing (8,588 filled)\n",
            "\n",
            "[Step 4] Filling missing barcodes...\n",
            "   Barcodes: 171 â†’ 130 missing (41 filled)\n",
            "\n",
            "[Step 5] Filtering by target ì†Œë¶„ë¥˜: ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']...\n",
            "   Rows after filtering: 72,789\n",
            "   Unique products: 221\n",
            "\n",
            "[Step 6] Advanced classification filling...\n",
            "   ëŒ€ë¶„ë¥˜: 0 â†’ 0 missing (0 filled)\n",
            "\n",
            "[Step 7] Cleaning up columns...\n",
            "   Dropped 8 columns\n",
            "   Remaining columns: ['íŒë§¤ì¼', 'íŒë§¤ìˆ˜ëŸ‰', 'ìƒí’ˆëª…', 'ì¤‘ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ê³µê¸‰ê°€ì•¡', 'ë¶€ê°€ì„¸', 'ë¶„ë¥˜í‚¤']\n",
            "\n",
            "[Step 8] Final cleanup...\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Final dataset shape: (72789, 8)\n",
            "Rows: 72,789\n",
            "Columns: 8\n",
            "\n",
            "Missing values per column:\n",
            "íŒë§¤ì¼     0\n",
            "íŒë§¤ìˆ˜ëŸ‰    0\n",
            "ìƒí’ˆëª…     0\n",
            "ì¤‘ë¶„ë¥˜     0\n",
            "ì†Œë¶„ë¥˜     0\n",
            "ê³µê¸‰ê°€ì•¡    0\n",
            "ë¶€ê°€ì„¸     0\n",
            "ë¶„ë¥˜í‚¤     0\n",
            "dtype: int64\n",
            "\n",
            "Unique values per classification:\n",
            "  ì¤‘ë¶„ë¥˜: 2\n",
            "  ì†Œë¶„ë¥˜: 2\n",
            "\n",
            "Sample of processed sales data:\n",
            "          íŒë§¤ì¼ íŒë§¤ìˆ˜ëŸ‰                  ìƒí’ˆëª…  ì¤‘ë¶„ë¥˜   ì†Œë¶„ë¥˜    ê³µê¸‰ê°€ì•¡    ë¶€ê°€ì„¸  \\\n",
            "0  2021-01-31    1      (ë†ì‹¬)ì‹ ë¼ë©´ë©€í‹°120g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   20700   2070   \n",
            "1  2021-01-31    2     (ë†ì‹¬)ì•ˆì„±íƒ•ë©´ë©€í‹°125g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   36300   3630   \n",
            "2  2021-01-31    1      (ë†ì‹¬)ì‹ ë¼ë©´ë©€í‹°120g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   20700   2070   \n",
            "3  2021-01-31    1      (ë†ì‹¬)ì‹ ë¼ë©´ë©€í‹°120g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   20700   2070   \n",
            "4  2021-01-31   16     (ë†ì‹¬)ì•ˆì„±íƒ•ë©´ë©€í‹°125g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´  291200  29120   \n",
            "5  2021-01-31    2      (ë†ì‹¬)ì‹ ë¼ë©´ë©€í‹°120g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´    5100    510   \n",
            "6  2021-01-31    1      (ë†ì‹¬)ì‹ ë¼ë©´ë©€í‹°120g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   20700   2070   \n",
            "7  2021-01-31    2    (ë†ì‹¬)ìˆœí•œë„ˆêµ¬ë¦¬ë©€í‹°120g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´    6300    630   \n",
            "8  2021-01-31    4  (ë†ì‹¬)ì˜¬ë¦¬ë¸Œì§œíŒŒê²Œí‹°ë©€í‹°140g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   13400   1340   \n",
            "9  2021-01-31    1      (ë†ì‹¬)ì‹ ë¼ë©´ë©€í‹°120g*5  ë¼ë©´ë¥˜  ë´‰ì§€ë¼ë©´   20700   2070   \n",
            "\n",
            "             ë¶„ë¥˜í‚¤  \n",
            "0      ì‹ ë¼ë©´ë©€í‹°120g  \n",
            "1     ì•ˆì„±íƒ•ë©´ë©€í‹°125g  \n",
            "2      ì‹ ë¼ë©´ë©€í‹°120g  \n",
            "3      ì‹ ë¼ë©´ë©€í‹°120g  \n",
            "4     ì•ˆì„±íƒ•ë©´ë©€í‹°125g  \n",
            "5      ì‹ ë¼ë©´ë©€í‹°120g  \n",
            "6      ì‹ ë¼ë©´ë©€í‹°120g  \n",
            "7    ìˆœí•œë„ˆêµ¬ë¦¬ë©€í‹°120g  \n",
            "8  ì˜¬ë¦¬ë¸Œì§œíŒŒê²Œí‹°ë©€í‹°140g  \n",
            "9      ì‹ ë¼ë©´ë©€í‹°120g  \n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ ALL Bë¬¼ë¥˜ì„¼í„° PREPROCESSING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Purchase Data: 4,367 rows\n",
            "ğŸ“Š Sales Data: 72,789 rows\n",
            "\n",
            "ğŸ’¾ Output files:\n",
            "\n",
            "ğŸ¯ Filtered by ì†Œë¶„ë¥˜: ['íƒ„ì‚°ìŒë£Œ', 'ë´‰ì§€ë¼ë©´']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categories_unique = b_purchase_processed['ì¤‘ë¶„ë¥˜'].unique()\n",
        "print(categories_unique)\n",
        "\n",
        "categories_unique = b_sales_processed['ì¤‘ë¶„ë¥˜'].unique()\n",
        "print(categories_unique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVWVxKgcZGp6",
        "outputId": "f1b9286d-8340-4396-ad25-0baf5a09b352"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ë¼ë©´ë¥˜' 'ìŒë£Œ']\n",
            "['ë¼ë©´ë¥˜' 'ìŒë£Œ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('B_purchase')\n",
        "print(b_purchase_processed.info())\n",
        "print('<===================================================>')\n",
        "print('B_sales')\n",
        "print(b_sales_processed.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGlVbTG3ZIZZ",
        "outputId": "2f9352ed-3498-41a4-9c46-feeb5e28a536"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B_purchase\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4367 entries, 0 to 4366\n",
            "Data columns (total 8 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   ì¼ì         4367 non-null   object\n",
            " 1   ê³µê¸‰ì—…ì²´ ì½”ë“œ    4367 non-null   object\n",
            " 2   ê³µê¸‰ì—…ì²´ ìš°í¸ë²ˆí˜¸  4367 non-null   object\n",
            " 3   ìƒí’ˆëª…        4367 non-null   object\n",
            " 4   EA         4367 non-null   object\n",
            " 5   ì¤‘ë¶„ë¥˜        4367 non-null   object\n",
            " 6   ì†Œë¶„ë¥˜        4367 non-null   object\n",
            " 7   ë¶„ë¥˜í‚¤        4367 non-null   object\n",
            "dtypes: object(8)\n",
            "memory usage: 273.1+ KB\n",
            "None\n",
            "<===================================================>\n",
            "B_sales\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 72789 entries, 0 to 72788\n",
            "Data columns (total 8 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   íŒë§¤ì¼     72789 non-null  object\n",
            " 1   íŒë§¤ìˆ˜ëŸ‰    72789 non-null  object\n",
            " 2   ìƒí’ˆëª…     72789 non-null  object\n",
            " 3   ì¤‘ë¶„ë¥˜     72789 non-null  object\n",
            " 4   ì†Œë¶„ë¥˜     72789 non-null  object\n",
            " 5   ê³µê¸‰ê°€ì•¡    72789 non-null  object\n",
            " 6   ë¶€ê°€ì„¸     72789 non-null  object\n",
            " 7   ë¶„ë¥˜í‚¤     72789 non-null  object\n",
            "dtypes: object(8)\n",
            "memory usage: 4.4+ MB\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}